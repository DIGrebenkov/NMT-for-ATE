{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78078dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "import ahocorasick\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916984df",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_TOKEN = '▁<extra_id_0>'\n",
    "SEQ_MAX_LENGTH = 150\n",
    "\n",
    "USED_MODEL_NAME = './coint_rut5small_finetune_fulltrain_novalid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained (USED_MODEL_NAME)\n",
    "\n",
    "device = torch.device ('cuda' if torch.cuda.is_available () else 'cpu')\n",
    "model = MT5ForConditionalGeneration.from_pretrained (USED_MODEL_NAME)\n",
    "model.to (device)\n",
    "print (model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_splitter (text, delimiters = ['...', '.', '?!', '?', '!']):\n",
    "\n",
    "    delimiters_pattern = '|'.join (map (re.escape, delimiters))\n",
    "    paragraphs = re.split (f'(?<=\\n[ ]*)', text)\n",
    "    \n",
    "    sentences_with_indices = []\n",
    "    current_start_index = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "\n",
    "        sentences = re.split (f'(?<=[{delimiters_pattern}] )(?=[A-ZА-ЯЁ])', paragraph)\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            start_index = current_start_index\n",
    "            end_index = start_index + len (sentence)\n",
    "\n",
    "            sentences_with_indices.append ((sentence, (start_index, end_index)))\n",
    "\n",
    "            current_start_index = end_index\n",
    "    \n",
    "    return sentences_with_indices\n",
    "\n",
    "\n",
    "LABEL_PREFIX_TOKEN = '▁<extra_id_1>'\n",
    "def get_set (tensor, tokenizer = tokenizer):\n",
    "\n",
    "    separator = SEP_TOKEN\n",
    "\n",
    "    seq = tensor [tensor != 0]\n",
    "    seq = seq [seq != 1]\n",
    "    seq = seq [seq != - 100]\n",
    "    \n",
    "    txt = tokenizer.decode (seq)\n",
    "    res = set ([item.strip () for item in txt.split (separator)])\n",
    "\n",
    "    if len (res) > 1:\n",
    "        res -= set ([''])\n",
    "    res -= set ([LABEL_PREFIX_TOKEN[1:]])\n",
    "    if len (res) == 0: res |= set ([''])\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def one_finder (text, phrases):\n",
    "\n",
    "    if len (phrases) == 1 and '' in phrases:\n",
    "        return []\n",
    "\n",
    "    A = ahocorasick.Automaton ()\n",
    "    \n",
    "    for idx, phrase in enumerate (phrases):\n",
    "        A.add_word (phrase, (idx, phrase))\n",
    "    \n",
    "    A.make_automaton ()\n",
    "    \n",
    "    found = []\n",
    "    for end_index, (idx, phrase) in A.iter (text):\n",
    "        start_index = end_index - len (phrase) + 1\n",
    "\n",
    "        if start_index > 0 and text [start_index - 1].isalpha ():\n",
    "            continue\n",
    "        if end_index + 1 < len (text) and text [end_index + 1].isalpha ():\n",
    "            continue\n",
    "\n",
    "        found.append ((start_index, end_index + 1, phrase))\n",
    "    \n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_with_model (texts, model = model, tokenizer = tokenizer):\n",
    "\n",
    "    model.eval ()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for text in tqdm (texts):\n",
    "\n",
    "        sentences_w_ind = raw_splitter (text)\n",
    "\n",
    "        answers = []\n",
    "        for sentence, (start, end) in sentences_w_ind:\n",
    "\n",
    "            sentence_tokenized = tokenizer (sentence, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "\n",
    "            with torch.no_grad ():\n",
    "\n",
    "                input_ids = sentence_tokenized ['input_ids'].to (model.device)\n",
    "                attention_mask = sentence_tokenized ['attention_mask'].to (model.device)\n",
    "\n",
    "                out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "\n",
    "                term_set = get_set (out [0])\n",
    "\n",
    "            found = one_finder (sentence, term_set)\n",
    "            answers += [[item [0] + start, item [1] + start] for item in found]\n",
    "\n",
    "        predictions.append (answers)\n",
    "\n",
    "    return (predictions)\n",
    "\n",
    "\n",
    "\n",
    "def predict_with_model_effective (texts, model = model, tokenizer = tokenizer):\n",
    "\n",
    "    model.eval ()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for text in tqdm (texts):\n",
    "\n",
    "        sentences_w_ind = raw_splitter (text)\n",
    "        sentences = [sent for sent, (_, _) in sentences_w_ind]\n",
    "\n",
    "        batch_size = 8\n",
    "        out = []\n",
    "        if len (sentences) > batch_size:\n",
    "            num_batches = (len (sentences) + batch_size - 1) // batch_size\n",
    "            for i in range (num_batches):\n",
    "                batch_sentences = sentences [i * batch_size: (i + 1) * batch_size]\n",
    "                \n",
    "                sentences_tokenized = tokenizer (batch_sentences, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "                \n",
    "                input_ids = sentences_tokenized ['input_ids'].to (model.device)\n",
    "                attention_mask = sentences_tokenized ['attention_mask'].to (model.device)\n",
    "                \n",
    "                output = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "                \n",
    "                out.extend ([item for item in output])\n",
    "        \n",
    "        else:\n",
    "            sentences_tokenized = tokenizer (sentences, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "                \n",
    "            input_ids = sentences_tokenized ['input_ids'].to (model.device)\n",
    "            attention_mask = sentences_tokenized ['attention_mask'].to (model.device)\n",
    "\n",
    "            out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "            out = [item for item in out]\n",
    "\n",
    "        answers = []\n",
    "        for i in range (len (out)):\n",
    "            sentence = sentences_w_ind [i] [0]\n",
    "            output = out [i]\n",
    "            start = sentences_w_ind [i] [1] [0]\n",
    "            term_set = get_set (output)\n",
    "\n",
    "            found = one_finder (sentence, term_set)\n",
    "\n",
    "            answers += [[item [0] + start, item [1] + start] for item in found]\n",
    "\n",
    "        predictions.append (answers)\n",
    "\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_constructor (labels):\n",
    "    res = []\n",
    "    for label in labels:\n",
    "        one_label = []\n",
    "        for start, end, cls in label:\n",
    "            one_label.append ([start, end])\n",
    "        res.append (one_label)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf46104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json ('./test_data/test1_t12_full_v2.jsonl', lines = True)\n",
    "df = df [['text', 'label']]\n",
    "print (df.head ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74969ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_txt = df ['text']\n",
    "val_labels_txt = df ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c52c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (val_data_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574031f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparator (pred, labl):\n",
    "\n",
    "    pred = set ([tuple (item) for item in pred])\n",
    "    labl = set ([tuple (item) for item in labl])\n",
    "\n",
    "    true_positives = len (pred & labl)\n",
    "    false_positives = len (pred - labl)\n",
    "    false_negatives = len (labl - pred)\n",
    "\n",
    "    return true_positives, false_positives, false_negatives\n",
    "\n",
    "def metricator (preds, labels):\n",
    "\n",
    "    tps_sum = 0\n",
    "    fps_sum = 0\n",
    "    fns_sum = 0 \n",
    "\n",
    "    for i in range (len (labels)):\n",
    "\n",
    "        true_positives, false_positives, false_negatives = comparator (preds [i], labels [i])\n",
    "\n",
    "        tps_sum += true_positives\n",
    "        fps_sum += false_positives\n",
    "        fns_sum += false_negatives\n",
    "\n",
    "    precision = tps_sum / (tps_sum + fps_sum) if (tps_sum + fps_sum) > 0 else 0\n",
    "    recall = tps_sum / (tps_sum + fns_sum) if (tps_sum + fns_sum) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_lst = val_data_txt.tolist ()\n",
    "val_labels_lst = label_constructor (val_labels_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd754ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_with_model_effective (val_data_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = metricator (predictions, val_labels_lst)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ae435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json ('./test2_t12_v2.jsonl', lines = True)\n",
    "print (df.head ())\n",
    "\n",
    "test_data_txt = df ['text']\n",
    "\n",
    "test_data_lst = test_data_txt.tolist ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc51a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_with_model (test_data_lst)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "print (test_data_lst [idx])\n",
    "for item in predictions [idx]:\n",
    "    print (f'{test_data_lst [idx] [item [0]: item [1]]}', end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df ['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f831f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json ('res-digr-test2_t12_v2.jsonl', orient = 'records', lines = True, force_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
