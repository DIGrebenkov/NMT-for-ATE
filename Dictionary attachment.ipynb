{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import ahocorasick\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, MT5ForConditionalGeneration, get_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "#from fairseq.optim.adafactor import Adafactor\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd2a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS:\n",
    "\n",
    "SEP_TOKEN = '▁<extra_id_0>'\n",
    "INPUT_PREFIX = ''\n",
    "\n",
    "NUM_OF_SPLITS = 50\n",
    "\n",
    "ATTACHMENT_TOKEN = '▁<extra_id_1>'\n",
    "LABEL_PREFIX_TOKEN = ATTACHMENT_TOKEN\n",
    "\n",
    "USED_MODEL_NAME = 'cointegrated/rut5-small'\n",
    "\n",
    "SEQ_MAX_LENGTH = 300\n",
    "ANSWER_MAX_LENGTH = 170\n",
    "BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 24\n",
    "\n",
    "ENABLE_LABEL_FIX = True\n",
    "\n",
    "LOCAL_NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328190fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained (USED_MODEL_NAME)\n",
    "\n",
    "device = torch.device ('cuda' if torch.cuda.is_available () else 'cpu')\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained (USED_MODEL_NAME)\n",
    "\n",
    "model.to (device)\n",
    "print (model.device)\n",
    "\n",
    "num_epochs = 50\n",
    "#training_steps = len (train_data ['input_ids']) // BATCH_SIZE * num_epochs\n",
    "#warmup_steps = int (training_steps * 0.1)\n",
    "#print (f'Suggested train steps: {training_steps}\\n\\t warmup steps: {int (training_steps * 0.05)} - {int (training_steps * 0.1)}')\n",
    "\n",
    "#optimizer = AdamW (model.parameters (), lr = 1e-5)#, weight_decay = 0.05)\n",
    "optimizer = AdamW (filter (lambda p: p.requires_grad, model.parameters ()), lr = 1e-4, weight_decay = 0.01)\n",
    "#optimizer = Adafactor (model.parameters (), lr = 3e-5, scale_parameter = False, relative_step = False, weight_decay = 0.02)\n",
    "#scheduler = get_scheduler ('linear', optimizer = optimizer, num_warmup_steps = warmup_steps, num_training_steps = training_steps)\n",
    "\n",
    "metrics = {'train_loss': [], 'val_loss': [], 'precision': [], 'recall': [], 'f1_score': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FUNCTIONS:\n",
    "\n",
    "def split_text (text, segments, delimiters = ['...', '.', '?!', '?', '!']):\n",
    "\n",
    "    delimiters_pattern = '|'.join (map (re.escape, delimiters))\n",
    "    \n",
    "    paragraphs = re.split (f'(?<=\\n[ ]*)', text)\n",
    "    \n",
    "    sentences_with_segments = []\n",
    "    \n",
    "    current_start_index = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "\n",
    "        sentences = re.split (f'(?<=[{delimiters_pattern}] )(?=[A-ZА-ЯЁ])', paragraph)\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            start_index = current_start_index\n",
    "            end_index = start_index + len (sentence)\n",
    "\n",
    "            matched_segments = [\n",
    "                text [start: end] for start, end in segments if start >= start_index and end <= end_index\n",
    "            ]\n",
    "            sentences_with_segments.append ((sentence, matched_segments))\n",
    "\n",
    "            current_start_index = end_index\n",
    "    \n",
    "    return sentences_with_segments\n",
    "\n",
    "def create_parallels (data_lst, labels_lst):\n",
    "    parallel_text = []\n",
    "    parallel_label = []\n",
    "    for i in range (len (data_lst)):\n",
    "        text = data_lst [i]\n",
    "        segments = labels_lst [i]\n",
    "        splitted = split_text (text, segments)\n",
    "        for sentence, terms in splitted:\n",
    "            parallel_text.append (INPUT_PREFIX + sentence)\n",
    "            constructed_label = LABEL_PREFIX_TOKEN\n",
    "            for term in terms:\n",
    "                constructed_label += SEP_TOKEN + term.strip ()\n",
    "            parallel_label.append (constructed_label)\n",
    "    return parallel_text, parallel_label\n",
    "\n",
    "\n",
    "def label_constructor (labels):\n",
    "    res = []\n",
    "    for label in labels:\n",
    "        one_label = []\n",
    "        for start, end, cls in label:\n",
    "            one_label.append ([start, end])\n",
    "        res.append (one_label)\n",
    "    return res\n",
    "\n",
    "\n",
    "def one_finder (text, phrases):\n",
    "\n",
    "    if len (phrases) == 1 and '' in phrases:\n",
    "        return []\n",
    "\n",
    "    A = ahocorasick.Automaton ()\n",
    "    \n",
    "    for idx, phrase in enumerate (phrases):\n",
    "        A.add_word (phrase, (idx, phrase))\n",
    "    \n",
    "    A.make_automaton ()\n",
    "    \n",
    "    found = []\n",
    "    for end_index, (idx, phrase) in A.iter (text):\n",
    "        start_index = end_index - len (phrase) + 1\n",
    "\n",
    "        if start_index > 0 and text [start_index - 1].isalpha ():\n",
    "            continue\n",
    "        if end_index + 1 < len (text) and text [end_index + 1].isalpha ():\n",
    "            continue\n",
    "\n",
    "        found.append ((start_index, end_index + 1, phrase))\n",
    "    \n",
    "    return found\n",
    "\n",
    "def make_attachment (text, dictionary_terms):\n",
    "    found = one_finder (text, dictionary_terms)\n",
    "    found_terms = [item [2] for item in found]\n",
    "    res = text + ATTACHMENT_TOKEN\n",
    "    for term in found_terms:\n",
    "        res += SEP_TOKEN + term\n",
    "    return res\n",
    "\n",
    "def attach_dictionary_foundings (texts, dictionary_terms):\n",
    "    res_texts = []\n",
    "    for text in tqdm (texts):\n",
    "        res_texts.append (make_attachment (text, dictionary_terms))\n",
    "    return res_texts\n",
    "\n",
    "def replace_padding (labels):\n",
    "    for i in range (0, len (labels ['input_ids'])):\n",
    "        labels ['input_ids'] [i] = torch.tensor ([labl if labl != 0 else - 100 for labl in labels ['input_ids'] [i]])\n",
    "\n",
    "    return labels\n",
    "\n",
    "class Seq2SeqDataset (Dataset):\n",
    "    def __init__ (self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__ (self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings ['input_ids'] [idx],\n",
    "            'attention_mask': self.encodings ['attention_mask'] [idx],\n",
    "            'labels': self.labels ['input_ids'] [idx]\n",
    "        }\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len (self.encodings ['input_ids'])\n",
    "\n",
    "def get_set (tensor, ground_truth = True, tokenizer = tokenizer):\n",
    "\n",
    "    separator: str\n",
    "    if SEP_TOKEN == '▁<extra_id_0>': separator = '<extra_id_0>'\n",
    "    else: separator = SEP_TOKEN\n",
    "\n",
    "    res: set\n",
    "\n",
    "    if ground_truth:\n",
    "        eos_idx = (tensor == 1).nonzero ()\n",
    "        if eos_idx.numel () > 0:\n",
    "            eos_idx = int (eos_idx [0] [0])\n",
    "        else:\n",
    "            eos_idx = len (tensor)\n",
    "        seq = tensor [:eos_idx]\n",
    "    \n",
    "    else:\n",
    "        seq = tensor [tensor != 0]\n",
    "        seq = seq [seq != - 100]\n",
    "        seq = seq [seq != 1]\n",
    "    \n",
    "    txt = tokenizer.decode (seq)\n",
    "    res = set ([item.strip () for item in txt.split (separator)])\n",
    "\n",
    "    if len (res) > 1:\n",
    "        res -= set ([''])\n",
    "    res -= set ([LABEL_PREFIX_TOKEN [1:]])\n",
    "    if len (res) == 0: res |= set ([''])\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def sanity_check (preds, labels, to_print = False):\n",
    "\n",
    "    tps_sum = 0\n",
    "    fps_sum = 0\n",
    "    fns_sum = 0 \n",
    "\n",
    "    for i in range (len (labels)):\n",
    "        predicted_set = get_set (preds [i], ground_truth = False)\n",
    "        true_set = get_set (labels [i])\n",
    "\n",
    "        if to_print: print (f'True: {true_set}\\nPred: {predicted_set}')\n",
    "\n",
    "        tps_sum += len (true_set & predicted_set)  # Истинно положительные\n",
    "        fps_sum += len (predicted_set - true_set)     # Ложноположительные\n",
    "        fns_sum += len (true_set - predicted_set)     # Ложноотрицательные\n",
    "\n",
    "    precision = tps_sum / (tps_sum + fps_sum) if (tps_sum + fps_sum) > 0 else 0\n",
    "    recall = tps_sum / (tps_sum + fns_sum) if (tps_sum + fns_sum) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json ('train_t1_v1.jsonl', lines = True)\n",
    "df.drop (columns = ['id', 'keywords'], inplace = True)\n",
    "print (df.head ())\n",
    "print ()\n",
    "df_2 = pd.read_json ('./test_data/test1_t12_full_v2.jsonl', lines = True)\n",
    "df_2 = df_2 [['text', 'label']]\n",
    "print (df_2.head ())\n",
    "print ()\n",
    "\n",
    "val_data_txt, test_data_txt, val_labels_txt, test_labels_txt = train_test_split (df_2 ['text'], df_2 ['label'], test_size = 0.5, random_state = 14)\n",
    "\n",
    "train_data_lst = df ['text'].tolist ()\n",
    "train_labels_lst = df ['label'].tolist ()\n",
    "train_parallel_text, train_parallel_labels = create_parallels (train_data_lst, train_labels_lst)\n",
    "\n",
    "val_data_lst = val_data_txt.tolist ()\n",
    "val_labels_lst = label_constructor (val_labels_txt)\n",
    "val_parallel_text, val_parallel_labels = create_parallels (val_data_lst, val_labels_lst)\n",
    "\n",
    "test_data_lst = test_data_txt.tolist ()\n",
    "test_labels_lst = label_constructor (test_labels_txt)\n",
    "test_parallel_text, test_parallel_labels = create_parallels (test_data_lst, test_labels_lst)\n",
    "\n",
    "all_train_terms = set ()\n",
    "for line in train_parallel_labels:\n",
    "    all_train_terms |= set (line.split (SEP_TOKEN))\n",
    "all_train_terms -= set ([''])\n",
    "all_train_terms -= set ([LABEL_PREFIX_TOKEN])\n",
    "print (f'Уникальных терминов во всём тренировочном наборе: {len (all_train_terms)}.\\n')\n",
    "\n",
    "val_parallel_text_new = attach_dictionary_foundings (val_parallel_text, all_train_terms)\n",
    "test_parallel_text_new = attach_dictionary_foundings (test_parallel_text, all_train_terms)\n",
    "\n",
    "val_data = tokenizer (val_parallel_text_new, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "val_labels = tokenizer (val_parallel_labels, padding = 'max_length', truncation = True, max_length = ANSWER_MAX_LENGTH, return_tensors = 'pt')\n",
    "\n",
    "test_data = tokenizer (test_parallel_text_new, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "test_labels = tokenizer (test_parallel_labels, padding = 'max_length', truncation = True, max_length = ANSWER_MAX_LENGTH, return_tensors = 'pt')\n",
    "\n",
    "if ENABLE_LABEL_FIX:\n",
    "    \n",
    "    replaced = replace_padding (val_labels)\n",
    "    val_labels = replaced\n",
    "\n",
    "    replaced = replace_padding (test_labels)\n",
    "    test_labels = replaced\n",
    "\n",
    "val_dataset = Seq2SeqDataset (val_data, val_labels)\n",
    "test_dataset = Seq2SeqDataset (test_data, test_labels)\n",
    "\n",
    "val_loader = DataLoader (val_dataset, batch_size = EVAL_BATCH_SIZE)\n",
    "test_loader = DataLoader (test_dataset, batch_size = EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_passed_epochs = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4331b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold (n_splits = NUM_OF_SPLITS)\n",
    "\n",
    "data = df ['text']\n",
    "labels = df ['label']\n",
    "\n",
    "test_indices = list (kf.split (data))\n",
    "test_indices.reverse ()\n",
    "\n",
    "inc_term_set = set ([''])\n",
    "for iteration, (train_index, test_index) in enumerate (test_indices):\n",
    "    X_curr = data [test_index]\n",
    "    y_curr = labels [test_index]\n",
    "\n",
    "    train_data_lst = X_curr.tolist ()\n",
    "    train_labels_lst = y_curr.tolist ()\n",
    "    parallel_text, parallel_labels = create_parallels (train_data_lst, train_labels_lst)\n",
    "\n",
    "    parallel_text_new = attach_dictionary_foundings (parallel_text, inc_term_set)\n",
    "\n",
    "    train_data = tokenizer (parallel_text_new, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "    train_labels = tokenizer (parallel_labels, padding = 'max_length', truncation = True, max_length = ANSWER_MAX_LENGTH, return_tensors = 'pt')\n",
    "    \n",
    "    if ENABLE_LABEL_FIX:\n",
    "        replaced = replace_padding (train_labels)\n",
    "        train_labels = replaced\n",
    "\n",
    "    train_dataset = Seq2SeqDataset (train_data, train_labels)\n",
    "    train_loader = DataLoader (train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "    for epoch in range (LOCAL_NUM_EPOCHS):\n",
    "        model.train ()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm (train_loader):\n",
    "            \n",
    "            input_ids = batch ['input_ids'].to (model.device)\n",
    "            attention_mask = batch ['attention_mask'].to (model.device)\n",
    "            labels_fromdata = batch ['labels'].to (model.device)\n",
    "\n",
    "            outputs = model (input_ids = input_ids, attention_mask = attention_mask, labels = labels_fromdata)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item ()\n",
    "\n",
    "            loss.backward ()\n",
    "            optimizer.step ()\n",
    "            optimizer.zero_grad ()\n",
    "\n",
    "            #scheduler.step ()\n",
    "            torch.cuda.empty_cache ()\n",
    "        \n",
    "\n",
    "        avg_loss = total_loss / len (train_loader)\n",
    "\n",
    "        metrics ['train_loss'].append ((epoch + iteration * LOCAL_NUM_EPOCHS + num_passed_epochs, avg_loss))\n",
    "\n",
    "        model.eval ()\n",
    "        val_preds, val_labels = [], []\n",
    "        total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad ():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch ['input_ids'].to (model.device)\n",
    "            attention_mask = batch ['attention_mask'].to (model.device)\n",
    "            labels_fromdata = batch ['labels'].to (model.device)\n",
    "\n",
    "            outputs = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "\n",
    "            total_val_loss += model (input_ids = input_ids, attention_mask = attention_mask, labels = labels_fromdata).loss.item ()\n",
    "\n",
    "            val_preds.extend (outputs)\n",
    "            val_labels.extend (labels_fromdata)\n",
    "            torch.cuda.empty_cache ()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len (val_loader)\n",
    "\n",
    "    prec, recl, f1sc = sanity_check (val_preds, val_labels)\n",
    "\n",
    "    metrics ['val_loss'].append ((epoch + iteration * LOCAL_NUM_EPOCHS + num_passed_epochs, avg_val_loss))\n",
    "    metrics ['precision'].append ((epoch + iteration * LOCAL_NUM_EPOCHS + num_passed_epochs, prec))\n",
    "    metrics ['recall'].append ((epoch + iteration * LOCAL_NUM_EPOCHS + num_passed_epochs, recl))\n",
    "    metrics ['f1_score'].append ((epoch + iteration * LOCAL_NUM_EPOCHS + num_passed_epochs, f1sc))\n",
    "\n",
    "    print (f'Epoch {(epoch + 1) + iteration * LOCAL_NUM_EPOCHS + num_passed_epochs} / {NUM_OF_SPLITS * LOCAL_NUM_EPOCHS + num_passed_epochs}, Loss: {avg_loss:.4f}, Validation loss: {avg_val_loss:.4f}, {prec} / {recl} / {f1sc}')\n",
    "    \n",
    "    torch.cuda.empty_cache ()\n",
    "\n",
    "\n",
    "    for line in parallel_labels:\n",
    "        inc_term_set |= set (line.split (SEP_TOKEN))\n",
    "    inc_term_set -= set ([''])\n",
    "    inc_term_set -= set ([LABEL_PREFIX_TOKEN])\n",
    "    print (f'Уникальных терминов в {iteration + 1}-м тренировочном наборе: {len (inc_term_set)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87808fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_passed_epochs += NUM_OF_SPLITS * LOCAL_NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911466b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, history) in enumerate (sorted (metrics.items ())):\n",
    "    #plt.plot (1, len (metrics), i + 1)\n",
    "    plt.figure (figsize = (10, 4))\n",
    "    plt.title (name)\n",
    "    plt.plot (*zip (*history))\n",
    "    plt.grid ()\n",
    "    plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_preds, val_labels_fromdata = [], []\n",
    "with torch.no_grad ():\n",
    "    for batch in tqdm (val_loader):\n",
    "        input_ids = batch ['input_ids'].to (model.device)\n",
    "        attention_mask = batch ['attention_mask'].to (model.device)\n",
    "        labels = batch ['labels'].to (model.device)\n",
    "        out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length=SEQ_MAX_LENGTH)\n",
    "\n",
    "        val_preds.extend (out)\n",
    "        val_labels_fromdata.extend (labels)\n",
    "        torch.cuda.empty_cache ()\n",
    "\n",
    "    print ('Validation wmax: ', sanity_check (val_preds, val_labels_fromdata))\n",
    "\n",
    "val_preds, val_labels_fromdata = [], []\n",
    "with torch.no_grad ():\n",
    "    for batch in tqdm (test_loader):\n",
    "        input_ids = batch ['input_ids'].to (model.device)\n",
    "        attention_mask = batch ['attention_mask'].to (model.device)\n",
    "        labels = batch ['labels'].to (model.device)\n",
    "        out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length=SEQ_MAX_LENGTH)\n",
    "\n",
    "        val_preds.extend (out)\n",
    "        val_labels_fromdata.extend (labels)\n",
    "        torch.cuda.empty_cache ()\n",
    "\n",
    "    print ('Test wmax: ', sanity_check (val_preds, val_labels_fromdata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df71c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained ('./rut5small_fulltrain_novalid_dictionary_postfix')\n",
    "#tokenizer.save_pretrained ('./rut5small_fulltrain_novalid_dictionary_postfix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
