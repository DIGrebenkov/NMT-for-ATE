{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78078dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Enfi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "import ahocorasick\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "from sklearn.model_selection import train_test_split   # not important for pipeline. only for test\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916984df",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_TOKEN = '▁<extra_id_0>'\n",
    "SEQ_MAX_LENGTH = 150\n",
    "\n",
    "#USED_MODEL_NAME = './saved_model_/coint_rut5small_finetune_ttsseed14_7722'\n",
    "#USED_MODEL_NAME = './coint_rut5small_finetune_6171-8031'\n",
    "#USED_MODEL_NAME = './coint_rut5small_finetune_fulltrain_novalid'\n",
    "USED_MODEL_NAME = './coint_rut5small_finetune_fulltrain_novalid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e45c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained (USED_MODEL_NAME)\n",
    "\n",
    "device = torch.device ('cuda' if torch.cuda.is_available () else 'cpu')\n",
    "model = MT5ForConditionalGeneration.from_pretrained (USED_MODEL_NAME)\n",
    "model.to (device)\n",
    "print (model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ededc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_splitter (text, delimiters = ['...', '.', '?!', '?', '!']):\n",
    "\n",
    "    delimiters_pattern = '|'.join (map (re.escape, delimiters))\n",
    "    paragraphs = re.split (f'(?<=\\n[ ]*)', text)\n",
    "    \n",
    "    sentences_with_indices = []\n",
    "    current_start_index = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "\n",
    "        sentences = re.split (f'(?<=[{delimiters_pattern}] )(?=[A-ZА-ЯЁ])', paragraph)#.strip ())\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            start_index = current_start_index\n",
    "            end_index = start_index + len (sentence)\n",
    "\n",
    "            sentences_with_indices.append ((sentence, (start_index, end_index)))\n",
    "\n",
    "            current_start_index = end_index\n",
    "    \n",
    "    return sentences_with_indices\n",
    "\n",
    "\n",
    "LABEL_PREFIX_TOKEN = '▁<extra_id_1>'\n",
    "def get_set (tensor, tokenizer = tokenizer):\n",
    "\n",
    "    separator = SEP_TOKEN\n",
    "\n",
    "    seq = tensor [tensor != 0]\n",
    "    seq = seq [seq != 1]\n",
    "    seq = seq [seq != - 100]\n",
    "    \n",
    "    txt = tokenizer.decode (seq)\n",
    "    res = set ([item.strip () for item in txt.split (separator)])\n",
    "\n",
    "    if len (res) > 1:\n",
    "        res -= set ([''])\n",
    "    res -= set ([LABEL_PREFIX_TOKEN[1:]])\n",
    "    if len (res) == 0: res |= set ([''])\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def one_finder (text, phrases):\n",
    "\n",
    "    if len (phrases) == 1 and '' in phrases:\n",
    "        return []\n",
    "\n",
    "    A = ahocorasick.Automaton ()\n",
    "    \n",
    "    for idx, phrase in enumerate (phrases):\n",
    "        A.add_word (phrase, (idx, phrase))\n",
    "    \n",
    "    A.make_automaton ()\n",
    "    \n",
    "    found = []\n",
    "    for end_index, (idx, phrase) in A.iter (text):\n",
    "        start_index = end_index - len (phrase) + 1\n",
    "\n",
    "        if start_index > 0 and text [start_index - 1].isalpha ():\n",
    "            continue\n",
    "        if end_index + 1 < len (text) and text [end_index + 1].isalpha ():\n",
    "            continue\n",
    "\n",
    "        found.append ((start_index, end_index + 1, phrase))\n",
    "    \n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e418a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_with_model (texts, model = model, tokenizer = tokenizer):\n",
    "\n",
    "    model.eval ()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for text in tqdm (texts):\n",
    "\n",
    "        sentences_w_ind = raw_splitter (text)\n",
    "\n",
    "        answers = []\n",
    "        for sentence, (start, end) in sentences_w_ind:\n",
    "\n",
    "            sentence_tokenized = tokenizer (sentence, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "\n",
    "            with torch.no_grad ():\n",
    "\n",
    "                input_ids = sentence_tokenized ['input_ids'].to (model.device)\n",
    "                attention_mask = sentence_tokenized ['attention_mask'].to (model.device)\n",
    "\n",
    "                #out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "                #out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "                out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "\n",
    "                term_set = get_set (out [0])\n",
    "                #print (term_set)\n",
    "\n",
    "            found = one_finder (sentence, term_set)\n",
    "            answers += [[item [0] + start, item [1] + start] for item in found]\n",
    "\n",
    "        predictions.append (answers)\n",
    "\n",
    "    return (predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_with_model_effective (texts, model = model, tokenizer = tokenizer):\n",
    "\n",
    "    model.eval ()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for text in tqdm (texts):\n",
    "\n",
    "        sentences_w_ind = raw_splitter (text)\n",
    "        sentences = [sent for sent, (_, _) in sentences_w_ind]\n",
    "\n",
    "        batch_size = 8\n",
    "        out = []\n",
    "        if len (sentences) > batch_size:\n",
    "            num_batches = (len (sentences) + batch_size - 1) // batch_size\n",
    "            for i in range (num_batches):\n",
    "                batch_sentences = sentences [i * batch_size: (i + 1) * batch_size]\n",
    "                \n",
    "                sentences_tokenized = tokenizer (batch_sentences, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "                \n",
    "                input_ids = sentences_tokenized ['input_ids'].to (model.device)\n",
    "                attention_mask = sentences_tokenized ['attention_mask'].to (model.device)\n",
    "                \n",
    "                output = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "                \n",
    "                out.extend ([item for item in output])\n",
    "        \n",
    "        else:\n",
    "            sentences_tokenized = tokenizer (sentences, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "                \n",
    "            input_ids = sentences_tokenized ['input_ids'].to (model.device)\n",
    "            attention_mask = sentences_tokenized ['attention_mask'].to (model.device)\n",
    "\n",
    "            out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "            out = [item for item in out]\n",
    "\n",
    "        answers = []\n",
    "        for i in range (len (out)):\n",
    "            sentence = sentences_w_ind [i] [0]\n",
    "            output = out [i]\n",
    "            start = sentences_w_ind [i] [1] [0]\n",
    "            term_set = get_set (output)\n",
    "\n",
    "            found = one_finder (sentence, term_set)\n",
    "\n",
    "            answers += [[item [0] + start, item [1] + start] for item in found]\n",
    "\n",
    "        predictions.append (answers)\n",
    "\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f92d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_constructor (labels):\n",
    "    res = []\n",
    "    for label in labels:\n",
    "        one_label = []\n",
    "        for start, end, cls in label:\n",
    "            one_label.append ([start, end])\n",
    "        res.append (one_label)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf46104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  АВТОМАТИЧЕСКИЙ АНАЛИЗ ТОНАЛЬНОСТИ ТЕКСТОВ НА О...   \n",
      "1  InBASE: ТЕХНОЛОГИЯ ПОСТРОЕНИЯ ЕЯ-ИНТЕРФЕЙСОВ К...   \n",
      "2  Выражение уважительности с помощью личных мест...   \n",
      "3  ДА ЧЕРТ ЛИ В ДЕТАЛЯХ?.. МЕРА ДЛЯ ОЦЕНКИ СОВПАД...   \n",
      "4  КОРПУСНАЯ ОЦЕНКА СОЧЕТАЕМОСТИ СЛОВ С ИСПОЛЬЗОВ...   \n",
      "\n",
      "                                               label  \n",
      "0  [[0, 33, specific], [22, 33, specific], [52, 7...  \n",
      "1  [[0, 6, nomen], [19, 44, specific], [30, 44, s...  \n",
      "2  [[0, 24, specific], [35, 53, specific], [42, 5...  \n",
      "3  [[51, 70, specific], [61, 70, specific], [116,...  \n",
      "4  [[0, 16, specific], [17, 34, specific], [30, 3...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json ('./test_data/test1_t12_full_v2.jsonl', lines = True)\n",
    "df = df [['text', 'label']]\n",
    "print (df.head ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74969ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_txt = df ['text']\n",
    "val_labels_txt = df ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c52c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      АВТОМАТИЧЕСКИЙ АНАЛИЗ ТОНАЛЬНОСТИ ТЕКСТОВ НА О...\n",
      "1      InBASE: ТЕХНОЛОГИЯ ПОСТРОЕНИЯ ЕЯ-ИНТЕРФЕЙСОВ К...\n",
      "2      Выражение уважительности с помощью личных мест...\n",
      "3      ДА ЧЕРТ ЛИ В ДЕТАЛЯХ?.. МЕРА ДЛЯ ОЦЕНКИ СОВПАД...\n",
      "4      КОРПУСНАЯ ОЦЕНКА СОЧЕТАЕМОСТИ СЛОВ С ИСПОЛЬЗОВ...\n",
      "                             ...                        \n",
      "108    ВЛИЯНИЕ ОБЪЕМА ОПЕРАТИВНОЙ ПАМЯТИ НА ИНТЕРПРЕТ...\n",
      "109    АНАЛИЗ ПАРАМЕТРОВ РЕЧЕВОГО СИГНАЛА СОЗДАЮЩИХ В...\n",
      "110    СТАБИЛЬНОСТЬ ИСТОЧНИКОВ КАК ОДИН ИЗ ПАРАМЕТРОВ...\n",
      "111    О ГРАММАТИКЕ КОНЦЕПТУАЛЬНЫХ ОТНОШЕНИЙ\\nВ рамка...\n",
      "112    МЕТОД КОНТЕКСТНОГО РАЗРЕШЕНИЯ ФУНКЦИОНАЛЬНОЙ О...\n",
      "Name: text, Length: 113, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (val_data_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "574031f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparator (pred, labl):\n",
    "\n",
    "    pred = set ([tuple (item) for item in pred])\n",
    "    labl = set ([tuple (item) for item in labl])\n",
    "\n",
    "    true_positives = len (pred & labl)\n",
    "    false_positives = len (pred - labl)\n",
    "    false_negatives = len (labl - pred)\n",
    "\n",
    "    return true_positives, false_positives, false_negatives\n",
    "\n",
    "def metricator (preds, labels):\n",
    "\n",
    "    tps_sum = 0\n",
    "    fps_sum = 0\n",
    "    fns_sum = 0 \n",
    "\n",
    "    for i in range (len (labels)):\n",
    "\n",
    "        true_positives, false_positives, false_negatives = comparator (preds [i], labels [i])\n",
    "\n",
    "        tps_sum += true_positives\n",
    "        fps_sum += false_positives\n",
    "        fns_sum += false_negatives\n",
    "\n",
    "    precision = tps_sum / (tps_sum + fps_sum) if (tps_sum + fps_sum) > 0 else 0\n",
    "    recall = tps_sum / (tps_sum + fns_sum) if (tps_sum + fns_sum) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "273e6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_lst = val_data_txt.tolist ()\n",
    "val_labels_lst = label_constructor (val_labels_txt)\n",
    "\n",
    "#test_data_lst = test_data_txt.tolist ()\n",
    "#test_labels_lst = test_labels_txt.tolist ()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd754ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 61/113 [00:26<00:22,  2.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_model_effective\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data_lst\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 80\u001b[0m, in \u001b[0;36mpredict_with_model_effective\u001b[1;34m(texts, model, tokenizer)\u001b[0m\n\u001b[0;32m     78\u001b[0m output \u001b[38;5;241m=\u001b[39m out [i]\n\u001b[0;32m     79\u001b[0m start \u001b[38;5;241m=\u001b[39m sentences_w_ind [i] [\u001b[38;5;241m1\u001b[39m] [\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 80\u001b[0m term_set \u001b[38;5;241m=\u001b[39m \u001b[43mget_set\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m found \u001b[38;5;241m=\u001b[39m one_finder (sentence, term_set)\n\u001b[0;32m     84\u001b[0m answers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [[item [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m start, item [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m start] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m found]\n",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m, in \u001b[0;36mget_set\u001b[1;34m(tensor, tokenizer)\u001b[0m\n\u001b[0;32m     31\u001b[0m seq \u001b[38;5;241m=\u001b[39m seq [seq \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     32\u001b[0m seq \u001b[38;5;241m=\u001b[39m seq [seq \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m---> 34\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m ([item\u001b[38;5;241m.\u001b[39mstrip () \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m txt\u001b[38;5;241m.\u001b[39msplit (separator)])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m (res) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Enfi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3813\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3788\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   3804\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3805\u001b[0m             seq,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3810\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3811\u001b[0m     ]\n\u001b[1;32m-> 3813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3815\u001b[0m     token_ids: Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3816\u001b[0m     skip_special_tokens: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3817\u001b[0m     clean_up_tokenization_spaces: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3818\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3819\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3821\u001b[0m \u001b[38;5;124;03m    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;124;03m    tokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3838\u001b[0m \u001b[38;5;124;03m        `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m   3839\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m     \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = predict_with_model_effective (val_data_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = metricator (predictions, val_labels_lst)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bec0c",
   "metadata": {},
   "source": [
    "### My own leader models\n",
    "\n",
    "name | metrics | smth\n",
    "-----|---------|----------\n",
    "./coint_rut5small_finetune_fulltrain_novalid | (0.7016524423461049, 0.7734187349879904, 0.7357897743501857) | \n",
    "./coint_rut5-base-absum_finetune_5982-7597 | (0.6875249103228378, 0.6905524419535628, 0.6890353505092869) | \n",
    "./coint_rut5small_finetune_6171-8031 | (0.6939553457977854, 0.7652121697357886, 0.7278438838648262) | \n",
    "./coint_rut5small_finetune_6077-7948 | (0.6774367259019924, 0.7554043234587671, 0.7142992334626669) |\n",
    "./coint_rut5small_finetune_ttsseed14_7722 | (0.6761090326028861, 0.7596076861489192, 0.7154302950325194) | \n",
    "BSET | 0.7035753374680773, 0.7689393939393939, 0.7348066298342542 | \n",
    "rut5small_7332-7420 | 0.7447399761810242, 0.7510008006405124, 0.7478572852302173\n",
    "rut5small_foldlearn_250ep_68-71 | (0.6639388009251023, 0.7469975980784628, 0.7030234529528115)\n",
    "rut5small_fulltrain_dictpostfix_avg7209-7591 | (0.6214299405788768, 0.6489191353082466, 0.6348771173993929)\n",
    "rut5small_fulltrain_folds_f1opt_avg--------- | (0.6287981093855503, 0.7455964771817454, 0.6822344322344321)\n",
    "rut5small_fulltrain_folds_f1opt_avg-v2-7250-7633- | (0.6041174485318933, 0.7165732586068855, 0.6555575901849477)\n",
    "rut5small_fulltrain_novalid_dictionary_postfix | (0.7344150298889838, 0.344275420336269, 0.4687925865358408) | no postfix\n",
    "rut5small_fulltrain100ep_novalid_dictionary_postfix | (0.695127402771569, 0.3112489991993595, 0.4299737315083645) | no postfix\n",
    "rut5small_posttrain_folds_f1opt_avg-7531-7816- | (0.6763404707131481, 0.7650120096076861, 0.717948717948718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_counter = 0\n",
    "\n",
    "def predict_with_model_effective_prints (texts, model = model, tokenizer = tokenizer):\n",
    "\n",
    "    global unmatched_counter\n",
    "\n",
    "    model.eval ()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for text in tqdm (texts):\n",
    "\n",
    "        sentences_w_ind = raw_splitter (text)\n",
    "        sentences = [sent for sent, (_, _) in sentences_w_ind]\n",
    "\n",
    "        batch_size = 8\n",
    "        out = []\n",
    "        if len (sentences) > batch_size:\n",
    "            num_batches = (len (sentences) + batch_size - 1) // batch_size\n",
    "            for i in range (num_batches):\n",
    "                batch_sentences = sentences [i * batch_size: (i + 1) * batch_size]\n",
    "                \n",
    "                sentences_tokenized = tokenizer (batch_sentences, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "                \n",
    "                input_ids = sentences_tokenized ['input_ids'].to (model.device)\n",
    "                attention_mask = sentences_tokenized ['attention_mask'].to (model.device)\n",
    "                \n",
    "                output = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "                \n",
    "                out.extend ([item for item in output])\n",
    "        \n",
    "        else:\n",
    "            sentences_tokenized = tokenizer (sentences, padding = 'max_length', truncation = True, max_length = SEQ_MAX_LENGTH, return_tensors = 'pt')\n",
    "                \n",
    "            input_ids = sentences_tokenized ['input_ids'].to (model.device)\n",
    "            attention_mask = sentences_tokenized ['attention_mask'].to (model.device)\n",
    "\n",
    "            out = model.generate (input_ids = input_ids, attention_mask = attention_mask, max_length = SEQ_MAX_LENGTH)\n",
    "            out = [item for item in out]\n",
    "\n",
    "        answers = []\n",
    "        for i in range (len (out)):\n",
    "            sentence = sentences_w_ind [i] [0]\n",
    "            output = out [i]\n",
    "            start = sentences_w_ind [i] [1] [0]\n",
    "            term_set = get_set (output)\n",
    "\n",
    "            found = one_finder (sentence, term_set)\n",
    "\n",
    "            mfoundar = set ([item [2] for item in found])\n",
    "            unmatched_counter += len (term_set - mfoundar)\n",
    "            print (f'>>>>В строке: {sentence}')\n",
    "            print (f'  Не найдены: {[item for item in term_set - mfoundar]}')\n",
    "\n",
    "            answers += [[item [0] + start, item [1] + start] for item in found]\n",
    "\n",
    "        predictions.append (answers)\n",
    "\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbae0b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/113 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'unmatched_counter' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_model_effective_prints\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data_lst\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 49\u001b[0m, in \u001b[0;36mpredict_with_model_effective_prints\u001b[1;34m(texts, model, tokenizer)\u001b[0m\n\u001b[0;32m     46\u001b[0m found \u001b[38;5;241m=\u001b[39m one_finder (sentence, term_set)\n\u001b[0;32m     48\u001b[0m mfoundar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m ([item [\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m found])\n\u001b[1;32m---> 49\u001b[0m unmatched_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m (term_set \u001b[38;5;241m-\u001b[39m mfoundar)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>В строке: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  Не найдены: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[item\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mitem\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mterm_set\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mmfoundar]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'unmatched_counter' referenced before assignment"
     ]
    }
   ],
   "source": [
    "predictions = predict_with_model_effective_prints (val_data_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070051c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6291f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ae435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json ('./test2_t12_v2.jsonl', lines = True)\n",
    "print (df.head ())\n",
    "\n",
    "test_data_txt = df ['text']\n",
    "\n",
    "test_data_lst = test_data_txt.tolist ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc51a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_with_model (test_data_lst)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "print (test_data_lst [idx])\n",
    "for item in predictions [idx]:\n",
    "    print (f'{test_data_lst [idx] [item [0]: item [1]]}', end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df ['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f831f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json ('res-digr-test2_t12_v2-moretrain.jsonl', orient = 'records', lines = True, force_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions = predict_with_model (test_data_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = metricator (predictions, test_labels_lst)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6058fae",
   "metadata": {},
   "source": [
    "max_length | val metrics | test metrics\n",
    "-----------|-------------|---------------\n",
    "50  | (0.7762276785714286, 0.762609649122807, 0.7693584070796461) | (0.7506617257808365, 0.7611379495437467, 0.7558635394456289)\n",
    "100 | (0.7714884696016772, 0.8070175438596491, 0.7888531618435156) | (0.7517552657973922, 0.8046162104133119, 0.7772880477054704)\n",
    "150 | (0.7723958333333333, 0.8130482456140351, 0.7922008547008546) | (0.7521281922884326, 0.8062265163714439, 0.7782383419689118)\n",
    "200 | (0.7725143154606976, 0.8135964912280702, 0.7925233644859813) | (0.7521281922884326, 0.8062265163714439, 0.7782383419689118)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34ac9a",
   "metadata": {},
   "source": [
    "params | val metrics | test metrics\n",
    "-------|-------------|---------------\n",
    "baseline | (0.7723958333333333, 0.8130482456140351, 0.7922008547008546) | (0.7521281922884326, 0.8062265163714439, 0.7782383419689118)\n",
    "do_sample = True | (0.7603661820140011, 0.7741228070175439, 0.7671828307525129) | (0.745850622406639, 0.7718733225979603, 0.7586388815615933)\n",
    "top_p = 0.95 | (0.7733405288720993, 0.7856359649122807, 0.779439760674463) | (0.751922091235264, 0.7874396135265701, 0.7692711064499212)\n",
    "top_p = 0.9 | (0.7749057619816909, 0.7889254385964912, 0.7818527574028797) | (0.7537198563365829, 0.7885131508319914, 0.7707240293809026)\n",
    "top_p = 0.85 | (0.7633262260127932, 0.7850877192982456, 0.774054054054054) | (0.7581060216160577, 0.7906602254428341, 0.7740409879138204)\n",
    "top_p = 0.8 | (0.7774813233724653, 0.7987938596491229, 0.7879935100054083) | (0.7516472377090725, 0.7960279119699409, 0.7732012513034411)\n",
    " |  | \n",
    "top_k = 5 | (0.7621359223300971, 0.774671052631579, 0.768352365415987) | (0.748829953198128, 0.7729468599033816, 0.7606973058637084)\n",
    "top_k = 10 | (0.7706868577609519, 0.78125, 0.7759324802613667) | (0.7476780185758514, 0.7777777777777778, 0.7624309392265193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQ_LENGTH (0.7521281922884326, 0.8062265163714439, 0.7782383419689118)\n",
    "# 150        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d33bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "print (val_data_lst [idx])\n",
    "\n",
    "pred_idx = predict_with_model ([val_data_lst [idx]])\n",
    "\n",
    "print (metricator (pred_idx, [val_labels_lst [idx]]))\n",
    "\n",
    "for item in pred_idx [0]:\n",
    "    print (f'{val_data_lst [idx] [item [0]: item [1]]}', end = ', ')\n",
    "print ()\n",
    "for item in val_labels_lst [idx]:\n",
    "    print (f'{val_data_lst [idx] [item [0]: item [1]]}', end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea06d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_constructor (labels):\n",
    "    res = []\n",
    "    for label in labels:\n",
    "        one_label = []\n",
    "        for start, end, cls in label:\n",
    "            one_label.append ([start, end])\n",
    "        res.append (one_label)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddffb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_json ('./test_data/dev.json', lines = True)\n",
    "\n",
    "#df = pd.read_json ('train_t1_v1.jsonl', lines = True)\n",
    "df = pd.read_json ('cl-ruterm3-sample.json')\n",
    "df = df [['text', 'label']]\n",
    "print (df.head ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9544682",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst = df ['text'].tolist ()\n",
    "labels_lst = df ['label'].tolist ()\n",
    "\n",
    "labels_lst = label_constructor (labels_lst)\n",
    "labels_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_with_model (data_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a212b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = metricator (predictions, labels_lst)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282210a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
